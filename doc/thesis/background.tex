\chapter{Background and Motivation} \label{chapter:background}
The key insight for achieving good protocol design is understanding the two most crucial themes that should underpin the development process: \textit{simplicity} and \textit{modularity} \cite{ProtocolDesign}.

Simplicity refers to the idea that a well designed protocol will often be made up of many smaller pieces.
In order to properly understand the working of the protocol, at large, it should suffice to understand the individual pieces, in isolation, together with the respective interactions between them.
Moreover, these pieces should be specialized; they should deliver one clear functionality, which should be easy to potentially verify and guarantee.

Modularity, on the other hand, is supposed to complement simplicity. The concept implies that, in building a complex feature, it should require multiple smaller individual pieces.
However, these should function as black boxes, assuming a well specified input and output, thus work perfectly well in isolation, to the extent that they can be both developed, maintained or replaced, accordingly, without affecting the integrity of the whole.

Overall, this section will highlight how the above two points, and other ideas on good protocol design, shaped the motivation and choices of this thesis.

\section{Choice of Protocol: Nigori}
When we began work on this thesis, we decided to choose Nigori for our protocol investigation for several reasons.
On the one hand, it is a fairly new protocol, still under active developement, yet that has gone through several iterations already, as can be seen in Figure \ref{fig:timeline}.
The project was initially started at Google, with a draft specification by Ben Laurie in 2010 \cite{NigoriDraft} and an initial proof--of--concept implementation in python, coupled with a later introduction into the Chrome browser for private data synchronization.
It proposed a security protocol, and an implicit system revolving around it, that would allow users to login to a server and use it for data storage, while encrypting the content such that the server would be unaware of what the data actually represented.
The timeline of the project later involved Alastair Beresford creating a Java version of the software in 2011, while at Google with an RFC \cite{NigoriRFC} of the protocol specification also in the works.
Also, in parallel with this thesis, a DART \cite{DART} / Javascript based version of the required client library for the protocol was being developed by Daniel Thomas, as well as an Android application and a Chrome browser extension.
Thus, together with the novelty and constant development aspect, Nigori also featured a lot of development across different platforms.

\myfig{Timeline of Nigori project 2010--2012.}{timeline}{0.9}

On the other hand, Nigori was interesting from the point of view of protocol analysis, as well, as previously mentioned.
As a security protocol, it incorporated and covered many of the core aspects of information security, despite its relatively young age.
We list some of these concepts below, with short descriptions, yet we highlight them more, as we go through the working of the protocol, per se.
\begin{description}
\item[Confidentiality] refers to the act of preventing disclore or discovery of information, both with regards to a user's identity, as well as his respective data.

\item[Integrity] is the property that ensures that data is accurately and consistently held, once provided, meaning it cannot be altered in any unauthorized or undetectable form.

\item[Availability] means having guaranteed access to data, on demand.

\item[Authenticity] refers to having communication that is genuine, done through valid messages and parties that actually are who they claim to be -- generally achieved through some form of content signing.
\end{description}

As a protocol, Nigori was envisioned out of the increasingly prevalent use of the cloud as a storage platform and the impact this has on the privacy of both users and their data.
Since the cloud can represent a completely heterogeneous group of storage machines, potentially completely decentralized and spread geographically, Nigori assumes that servers can get compromised.
Nevertheless, it only assumes that the servers are \textit{honest--but--curious} -- meaning that they might seek access to the stored data, but they must respect the protocol, as far as communication goes, otherwise failure occurs.
As such, from a security point of view, Nigori chooses to strongly enforce \textbf{confidentiality}.
It makes use of cryptographic measures to provide servers with encrypted data, protecting both a user's identity, as well as his data (secrets), as only the user should be able to decrypt the data.

Nevertheless, from such a privacy perspective, there are plenty of use cases in which this would be desirable.
For example, Google's Privacy Policy \cite{GooglePrivacy} states that while access to personal user data is prohibited, it is done in a best effort manner.
Moreover, automated tools already go through the data, such as Gmail messages, for ad targeting.
Similarly, Dropbox's Privacy Policy \cite{DropboxPolicy} also states that access to data is prohibited, however, automated tools do interpret the files, either for analytics or content deduplication.
Depending on the type of content a user would like to keep, this style of service could prove insufficient.

On a different note, a separate focus for Nigori, as a protocol, is on the increasing level of diversity presented to application developers.
In parallel with the adoption of using cloud storage, the number of different types of clients accessing it has also increased.
As such, we now have different native mobile OS applications, various traditional (PC/laptop) operating systems and their native applications, as well as various browser counterparts, all serving either as clients, accessing cloud data, or even as servers, hosting it.
In this context, being able to develop the core of a protocol, such as Nigori, once, and have it compiled and used consistently and with strong guarantees across platforms, becomes a very strong incentive.
With this in mind, Nigori was created with this client diversity at its core, assuming individual users potentially having different devices, running the same application on all of them, backed by a Nigori data store.
This has two main advantages.
On the one hand, Nigori can help serve as a single--signon mechanism, should it be guarding further passwords or cryptographic keys, requiring one single password entry for access to all of a user's private data, from any device.
On the other hand, this diversity contributes to the aforementioned \textbf{availability} aspect, as clients can make use of synchronization mechanisms to access data from various locations.
A perfect example of where Nigori is actually currently being used as such is inside Google Chrome, in the Sync functionality, which allows a user's browser (private) data to be shared across devices, in all instances of the browser that are attached to his Google account (one password).
Similar applications for single access to encrypted data also exist, such as 1Password \cite{OnePassword}, or PasswordBox \cite{PasswordBox}, which provide an encrypted store for a user's secrets (in this case, other passwords), without them ever leaving the local computer, and requiring that the user remember one single password.

In the following, we present Nigori in more detail, together with the protocol aspects that make it interesting, both from a security perspective, but also from a design point of view.
For an easier breakdown of functionality, we describe the two main parts, the client library and the (potentially) remote server in separation, while also linking across them.

\subsection{Client}
The two main responsibilities of the client are cryptography (encryption and decryption of data) and communication (valid messages for the interaction with the server).
On top of this, the client must also passively make use of distributed version control primitives, similar to systems such as \textit{git}, in order to guarantee that the user's data is always synchronized across the various devices a user may have.
The primary use of this is as conflict resolution mechanics -- through the \textit{App defined merge} step in Figure \ref{fig:sync} \footnote{Picture taken from the Nigori web page of the Cambridge Digital Technology Group \textit{http://www.cl.cam.ac.uk/research/dtg/nigori/}.}.
However, this feature also helps with data availability, as previously mentioned, by allowing data to be fetched from various data stores.
In this regard, Nigori was strongly influenced by the Bayou system \cite{Bayou} and their conclusive findings that, for content synchronization and achieving data consistency, the application defined merging and conflict resolution phases are paramount.
This is especially true in the context of having a choice between actively resolving conflicts through automatic developer defined procedures or marking the data as unavailable until conflicts get resolved, considering that the initial cause of the conflict might be a device that is not reachable at the respective time.
The latter is undesirable, considering the \textbf{avaiability} property of the protocol.

\myfig{Synchronization and separation of trust in Nigori.}{sync}{1}

The above considerations make the protocol highly client--centric, with both data security and conflict resolution placed on the user end.
However, this ends up being a positive aspect, as implicitly, the user gets to protect his data from any other party interested in reading it.
Nevertheless, this control comes with an inherent set of responsibilities.
The client must make proper use of certain cryptographic primitives, as described further -- and given in more detail in the protocol specification.

\begin{description}
  \item[\myref{SHA-1}] is a secure hash function that generates a 160 bit message digest, as specified in RFC3174 \cite{RFC3174}.
  It is used in Nigori either internally, in other primitives, or directly, to shorten long messages to a fixed size format, with small probability of collisions.

  \item[\myref{HMAC}] represents the hash--based message authentication code (MAC) of a message, under a certain key, as specified in RFC2104 \cite{RFC2104}.
  As with any MAC, it is used to verify both the integrity, as well as the authenticity of a certain message.
  This primitive is the primary enforcer of the \textbf{integrity} property of the protocol's data.
  Implicitly, \myref{SHA-1} is the hash function used.

  \item[\myref{DSA}] represents the Digital Signature Algorithm, as specified by the FIPS186 document \cite{DSA}.
  The algorithm generates a pair of public and private keys.
  These are used afterwards to sign a message on the client and verify the respective signature (and thus, provide the \textbf{authenticity} property of the protocol) on the server.
  The Nigori specification defines fixed parameters for obtaining a public key of size 3072 bits and a private key of 128 bits.

  \item[\myref{AES}] is a symmetric--key encryption algorithm, as specified by the FIPS197 document \cite{AES}.
  By its nature, it allows the same cryptographic key to be used for both encryption and decryption.
  As such, the client is able to encrypt its data before sending it to the server and decrypt it when getting it back.
  However, the server, without the client's key, cannot perform either.

  \item[\myref{PBKDF}] is the password based key derivation function, as specified in its respective RFC2898 \cite{RFC2898}, under \myref{PBKDF2}.
  This takes an input password, adds a cryptographic salt and applies a secure hash function, such as \myref{SHA-1}, repeatedly, in order to generate values which are to later be used as cryptographic keys.
  A high number of iterations (over 1000) is to be used, to make it computationally intensive, and thus a difficult attack vector.
\end{description}

As mentioned, one motivation for Nigori was the idea of having the user provide one single password as input to the protocol -- a master password.
As such, by using \myref{PBKDF}, with appropriate parameters, Nigori generates a set of four cryptographic keys.
Each of the four respective keys has a well defined role in the protocol:

\begin{description}
  \item[\myref{kuser}] is used for authenticating the user on the server; it is used in such a way as to not allow potential dictionary attacks against the user's master password.
  \item[\myref{kenc}] is used for encrypting the user's secrets.
  \item[\myref{kmac}] is used for generating the MACs and implicitly authenticating the secrets.
  \item[\myref{kiv}] is used to deterministically generate an initialization vector (IV) given a plaintext, thus preventing common prefix attacks, when using deterministic encryption.
\end{description}

For an added element of security in creating the above keys, while the number of iterations and the length of the output key are fixed constants for \myref{PBKDF}, for each respective key, and the password is the user's master password, the salt is specifically created.
It is generated by applying \myref{PBKDF} once again to a concatenation of the username (as provided by the user) and servername, as the password, with the remaining parameters as fixed known constants.
This is done to make the actual key derivation process both user and server dependent.

For generating these above keys, the Nigori specification describes two methods, depending on how the system is setup.
The first one is called unassisted key derivation and it only requires the user presenting his master password for authentication at the server.
This has the drawback, however, that if the user forgets this password, all his data is subsequently lost (not decryptable), as there is no password recovery mechanism.
The second method involves an indirection layer through a third party service that will use the user's password to derive the key that will ultimately be used for the actual authentication to the Nigori backend.
As such, identity management is left to the third party, while the respective key to be used for contacting the Nigori data store cannot be lost.
For the scope of this thesis, only the first method has been implemented.

\myfig{Nigori two level index--value data model.}{datamodel}{1}

Upon generating the abovementioned four keys, the client can perform the required content encryption on upload and subsequent decryption on download.
However, before we detail the respective operations, we must first go over the actual data storage format available in Nigori, as seen in Figure \ref{fig:datamodel}.
This is crucial for both clients that want to store content as well as servers that (must) support respective functionality.
In effect, for a user, Nigori acts as a generic two--level index--value store.
Every user's stored data is a mapping from some indices to their respective revision history -- since, for conflict resolution, all data is versioned.
This history, in turn, is as well a mapping, from revision ID to the respective value.
As such, when a client uploads data, it is added under a certain index and, under that, a specific revision ID.
Also, at the moment, all fields can be arbitrary strings, of arbitrary length.

As such, since users wish to have all of their content encrypted, this means both indices, revisions and the actual data must be kept secret.
However, since for query purposes, the client must be able to consistently provide a respective index or revision, that encryption must be deterministic.
Nevertheless, for the actual data, that encryption can be non--deterministic.
Hence, the last pieces of cryptographic functionality represents two pairs of encrypt--decrypt functions, one that is deterministic (\myref{EncDet}, \myref{DecDet}), the other that is not (\myref{Enc}, \myref{Dec}).
They function by making use of \myref{AES} to encrypt the content and \myref{HMAC} to guarantee validity of messages.
The difference in determinism comes in the use of either a fixed (\myref{kiv}) or random IV for \myref{AES}.
These three components (IV, encrypted content, \myref{HMAC}) are then concatenated together, such that the server cannot use the pieces to decrypt the content, as it does not possess all the \myref{AES} parameters.
The client, on the other hand, being able to generate these, can always break the message down into its respective three components and decrypt the content, as \myref{AES} is a symmetric--key algorithm.
As such, through the use of \myref{AES}, the client strongly enforces the \textbf{confidentiality} property for the protocol, while the use of \myref{HMAC} on top of the content provides \textbf{integrity} guarantees to both the client and the server.

Lastly, the client best showcases the two main principles of good protocol design.
The individual cryptographic primitives act as the main pieces of functionality.
These can be abstracted away and implemented separately and potentially even formally verified for correctness.
On top of these, Nigori modularly builds the key generation methods as well as the encryption and decryption mechanics for user data.

\subsection{Server}
In describing the server, we can easily draw parallels between the respective pieces of functionality it needs to provide and the five constituent parts of a protocol \cite{ProtocolDesign}:
\begin{description}
  \item[Service] What it provides.
  \item[Assumptions] The environment in which it runs.
  \item[Vocabulary] Messages used for communication.
  \item[Encoding] Format of each message.
  \item[Procedure] Rules for consistency between message exchanges.
\end{description}

With regards to \textbf{service}, the server is primarily responsible for two things: properly authenticating users and safely storing their respective data.
However, to provide any type of functionality, the server must expose an interface, to allow communication.
This is achieved through the shared use of a Representational State Transfer (REST) \cite{REST} style interface.
The main idea behind such an interface involves a clear separation between client and server implementations and functional requirements.
Moreover, it assumes that the communication can be completely stateless, without the server needing to keep any extra information, across requests.
Thus, every new request made by a client must be fully self--contained.
There are several other requirements, yet these are the primary ones that are of interest to our server, in the context of this thesis.

Given this, the REST interface represents the underlying \textbf{assumptions} that the protocol makes about the environment in which it operates.
As such, the server provides a set of endpoints (explicit URL paths, relative to the base host name), corresponding to the various pieces of functionality that it exposes to clients.
Each endpoint expects a certain format of information, in order to properly do the functionality it promises.
The required format is abstracted away and described in the Nigori specification through the use of the actual protobuf \cite{protobuf} package that is used in the Java implementation.
A direct one--to--one mapping is thus established between the available endpoints and the messages that represent the expected payload format.
Overall, the provided interface acts much like remote procedural calls, with each function (HTTP requests at a certain endpoint) expecting a certain format of parameters (the matching message).

\begin{table}
  \centering
  \begin{tabular}{ |*{4}{l|} }
    \hline
    Type & \multicolumn{2}{l|}{Subtype} & Description \\ \hline
    \hline
    \multirow{3}{*}{\pbox{10cm}{User \\ management}} & \multicolumn{2}{l|}{Authenticate} & Authenticates the user. \\\cline{2-4}
    & \multicolumn{2}{l|}{Register} & Registers the user and creates a data store. \\\cline{2-4}
    & \multicolumn{2}{l|}{Unegister} & Unregisters the user and deletes all content. \\\hline
    \multirow{5}{*}{User data} & \multicolumn{2}{l|}{Put}& Uploads data for the user. \\\cline{2-4}
    & \multicolumn{2}{l|}{Delete}& Removes data for the user. \\\cline{2-4}
    & \multirow{3}{*}{Get} & Indices & Provides the first level of indices. \\\cline{3-4}
    & & Revisions & Provides a revision history. \\\cline{3-4}
    & & Data & Provides an explicit data item. \\\hline
  \end{tabular}
  \caption{Types of messages used for communication in Nigori.}
  \label{table:messages}
\end{table}

The various messages allowed in Nigori are primarily derived from the functionality that the server is expected to offer.
Moreover, while all requests expect a certain message format, some also deliver responses back to the client, which in turn are also a message, in the aforementioned sense.
Together, these messages describe the \textbf{vocabulary} used by Nigori.
As functionality goes, the messages are split into two main categories, shortly described in Table \ref{table:messages}.
We give a more thorough explanation of them in the following:
\begin{description}
  \item[User management] Since each server can keep track of data for multiple users, these messages are used for registering and unregistering users, as well as generic authentication.
  \begin{description}
    \item[Authenticate] The message must contain a hash of the user's DSA public key (for identifying the source), the servername (for identifying the destination), a one--time nonce, made of a timestamp and a random (four) byte string (to be checked for duplication, against replay attacks) and finally, the DSA signature of the entire message.
    This is the main mechanism through which the \textbf{authentication} property of the protocol is guaranteed on behalf of the client.

    \item[Register] The message should contain the public key the user wishes to be identified by and a token to be used for any potential administrative rights the server may configure (such as space quotas, ACLs or otherwise).

    \item[Unregister] The message must contain just an actual authentication message.
    This way, the endpoint can properly validate the identity of the user requesting the command and subsequently remove his content and identity from the store.
  \end{description}

  \item[User data] All commands that are related to the data of one specific user require authentication.
  This is achieved by embedding an authentication message inside whichever message is required.
  As such, though, the respective DSA signature must cover the entirety of the message, as to respect the validity guarantees it is meant to offer.
  Overall these commands are of three types:
  \begin{description}
    \item[Put] The main method of uploading content to a server.
    Considering the data format, a client is required to provide an index, a revision and a value.

    \item[Delete] The only method of modifying content already on the server is by removing it.
    This may be done on an explicit basis, by providing both an index and a revision, or generically by providing the index alone, thus removing all the revision history underneath.

    \item[Get] For accessing the data, there are multiple types of getter methods provided by the server.
    For each of these expected getter messages, a corresponding message format for the response is expected by the client.
    These getters are basically of three types:
    \begin{description}
      \item[Indices] These return all indices available for a specific user's data.
      \item[Revisions] These return all available revision IDs under a specific index.
      \item[Data] These return the actual data under a specific index.
      It can either be the entirety of the revision history, or one specific instance, if the revision is provided.
    \end{description}
  \end{description}
\end{description}

The aforementioned message formats also requires an extra set of notes.
At the moment, the specification requires all communication to be done across HTTPS, to ensure a secure channel.
This is the only way the \textbf{authentication} property of the protocol is kept, with regards to the server -- through a valid certificate.
What is more, every request must be an HTTP POST request, with the payload (body) being the actual message.
However, in order to communicate, the client and server must both understand the data they exchange.
As such, this data must be serialized to a common format.

The protocol specification offers two alternatives for serialization, text based JSON or binary protobuf, each with their pros and cons.
On the one hand, being a text based serialization method, JSON is directly available and understandable on any platform, while also greatly helping in the development process with cleartext debugging.
Moreover, it offers objects which can be directly manipulated in Javascript, for a respective client.
On the other hand, however, it requires data to be encoded such that certain characters are not available.
The protocol demands Base64 encoding, yet this comes at the price of extra space used by data, after encoding.
In contrast, using protobufs, the data is serialized in binary format, with increased performance.
However, this requires the technology to be available for both clients and servers, which is not always the case.
As such, while protobufs are favored, the server must always provide JSON.

Together with these final notes, the messages have a fully specified format, thus offering the \textbf{encoding} requirements of a well defined protocol.
Lastly, the functionality rules for each provided endpoint represent the overall \textbf{procedure} side of a protocol.
Because our server uses a REST interface, the overall set of rules is not overly complex, since each new request is stateless and all operations must be initiated by the client.

\section{Choice of environment: Mirage and OCaml}
As previously noted, the idea behind using Mirage is in its very definition as a library operating system (libOS).
The implications of this are three fold.
Firstly, it allows for any application being developed on top of it to be packaged together with all the underlying functionality required from the actual operating system into a single \textit{unikernel} that can be directly deployed on top of hypervisors, such as Xen, on top of cloud infrastructure VMs, such as Amazon EC2 machines.
Moreover, through the modular design of the entire project, each individual component, from the network stack protocols, to the application layer functionality, can be customized, as per the requirements of the application being deployed.
Secondly, this focus on minimization and modularization provides direct benefits to security.
Naturally, fewer components results in a smaller TCB, which makes for less overall attack vectors and potential entry points for malicious behavior.
Moreover, the selection of pieces of functionality is done in a semi--automated fashion, based on the actual explicit requirements of the application.
Lastly, Mirage can also be used as the sum of its constituent parts, its respective libraries.
As such, if application developers are targeting other types of deployments from the raw, unikernel approach, they can simply make use of the Mirage libraries and tailor their code towards compiling for the platform at hand.

On the security side of using Mirage, one extra implicit benefit gained is the use of the underlying programming language.
Mirage, the system and the implicit set of libraries, is writen in OCaml.
As such, while it is expected to eventually allow for bindings in other languages, at the moment, integration is best done by writing applications in OCaml.
This is important, as, from the beginning of the project, we wanted the implementation work to be done in a functional programming language with a static type system -- hence OCaml was fitting.
The motivations for this are two fold.
On the one hand, this opens up the protocol implementation, or at least for the crucial underlying pieces of it, to formal verification, which can directly vouchsafe certain protocol assumptions.
On the other hand, type systems directly provide the developer with a safety net, thus helping ensure that the theoretical design of the protocol at hand is not involuntarily broken during implementation.
Thus, abstract specifications of data and interactions can be turned into concrete data types and functions acting explicitly on them, with the language itself helping to enforce the constraints.
Overall, this should help increase the overall security of the protocol, which, considering the ideology behind Nigori, is important.
What is more, OCaml compiles down to native bytecode, which allows all the type enforcement to happen at compile time.
As such, after compilation, the type information can be removed, and, short of compiler bugs, this gives programs all the benefits of a type system, without a performance penalty.

Moreover, together with the aforementioned principles of simplicity and modularity, such a language helps put the problem of protocol design and implementation into a development funnel style picture, as in Figure \ref{fig:funnel}.
Thus, the base of the funnel consists of the many small individual pieces of protocol functionality, properly specified and potentially formally verified, guaranteed to offer the envisioned functionality, such as the cryptographic primitives in Nigori.
The top of the funnel is represented by the broad spectrum of final deployments of the protocol, depending on the targeted platform.
All of these would in a sense be variations, or specializations of the core of the protocol, the narrow waist of the funnel, which would represent the complete implementation of the protocol, achieved by modularly combining the functionality from lower down the funnel, in our language of choice, ready to be compiled away towards the needed platforms.

\myfig{Protocol development funnel.}{funnel}{0.4}

On the use of libraries for development and protocol prototyping, using Mirage as a backend and OCaml as the implicit language actually provides for good synnergy.
By default, Mirage comes with a package manager for OCaml which allows for certain libraries to be installed and used directly into applications built on top.
As such, we were able to leverage quite a great deal of functionality in engineering the Nigori implementation.
For example, for HTTP communication, we used the \myref{ocaml-cohttp} library's client and server, which provided us with plenty of direct and reliable functionality for the interactions.
With regards to the actual messages used in the communication, we used the \myref{atdgen} library, which allowed for \textit{Adjustable Type Definitions} to be written down and annotated for generating corresponding OCaml types.
What is more, the library also provided primitives for serializing and deserializing these types to and from JSON.
Certain cryptographic building blocks were directly available using the \myref{cryptokit} library, thus contributing to the primitives required by Nigori.
For storage, a backend Object Relational Mapping (ORM) was also available, by using the \myref{ocaml-orm} library.
This allows for carefully crafted OCaml types to be persisted using SQLite, in an almost seamless manner, with regards to the developer, while also allowing application specific OCaml code running directly on said types, without having to handle raw SQL.
Finally, through the use of the \myref{js\_of\_ocaml} library, we were able to also compile the OCaml client to Javascript, with appropriate adjustments.
All of these are explained in much greater detail in the remainder of the thesis.
