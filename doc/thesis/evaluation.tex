\chapter{Evaluation}

TODO: Split evaluation into two pieces: iterative process and produced system.

TODO: Iterative process: discuss about modifications brought back to the RFC for various reasons (clarity, insufficient information, erroneous pieces); show improvements brought forward toward the further development of the RFC (SSH like handshake for deciding encryption mechanics, fixed set of DSA parameters...)

TODO: Implementation: this part can stay similar, focusing on performance.

In light of the discussion with Anil, I'm not entirely sure how to structure the evaluation section.
Nevertheless, it should at least consist of a contrast between the OCaml Linux version and the previous Java implementation.
Moreover, performance could be also tested three--ways if the server backend would work directly in JS as well, for an in--memory storage part.

Another point in the evaluation section, considering cross platform work, would be to somehow look at the trusting computing base, yet, I am unsure how that could be evaluated.

What follows is the previous outline I had, considering testing Mirage on Xen:

This section should detail the experiments and the requirements for them.
For now, I am considering both storage and speed tests.
Depending on if and when we manage to setup Mirage on Xen and Linux on the same machine, to do comparative testing, the list might shorten and impact might be placed on a specific part.

With the system running on Mirage on Xen, I would like to see if we can test all three combinations.
However, if all else fails, we can always just test the OCaml version against the Java version, for the same machine.

\section{Storage}
\subsection{Memory}
\subsection{Disk Space}

\section{Speed}
\subsection{Latency}
\subsection{QPS}
