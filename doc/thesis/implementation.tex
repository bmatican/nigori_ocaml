\chapter{Design and Implementation} \label{chapter:implementation}
Nigori, as a system, is basically an encrypted index--value store, built for the cloud.
As such, the big picture consists of primarily two components: a client and a server.
Nevertheless, due to the nature of the interactions between these two pieces, quite a sizeable chunk of code ends up being shared, particularly the communication and some of the encryption mechanics.
This chapter is modeled after the actual implementation, showing the shared code, and the client and the server in separation, while also discussing the interactions between them.
In addition to describing the base OCaml implementation, this chapter also provides details about the separate compilation from OCaml code to JavaScript, along with both what it requires, as well as what it entails.

\section{Shared}
The corresponding section of the codebase for shared code actually represents the core of the application.
For the most part, components in this category are used by both the server and the client.
As such, they have been implemented according to the simplicity and modularity principles mentioned in the previous chapter.
Implicitly, this made using this core afterwards, in both client and server, a straightforward venture.
Moreover, it also allowed for an easy prototyping of the javascript version of the client, as well, since the core functionality was merely functorized to match the targeted platform.

\subsection{Crypto}
 As previously discussed, several cryptographic functions are required for the operation of the protocol.
These functions were built on top of the \myref{cryptokit} library for OCaml and abstracted into their individual modules, each capable of providing the expected functionality as well as being tested or verified accordingly.

As such, we had several base primitives that were required such as \myref{SHA-1}, \myref{HMAC}, \myref{AES}, \myref{DSA} and \myref{PBKDF}.
We use \myref{SHA-1} for hashing purposes, both implicitly (such as in \myref{PBKDF}), but also explicitly (for example, on \myref{DSA} keys in messages).
\myref{HMAC} is used solely on the encryption and decryption steps of indices, revisions or actual data, to guarantee data integrity and validity.
Similarly, \myref{DSA} is used primarily for authentication and verification.
In the client, we create the public / private key pairs for a particular user.
Afterwards, we generate and introduce a message signature, inside packets that are or contain an authentication message themselves.
Finally, on the server, the signature is verified, accordingly, using the user's public key, that is stored as part of his identity.
Lastly, \myref{PBKDF} is used, together with the users's master password, on the client, to generate specific keys to be used for user authentication, encryption, MACs or IV for \myref{AES}.

Of all of the above, \myref{SHA-1} and \myref{HMAC} came with the OCaml \myref{cryptokit} library and were used as--is.
\myref{AES} was also provided, however, using it (in \myref{Enc} and \myref{EncDet} and their decryption couterparts) required tweaking some parameters, as per the protocol specification.
The \myref{PBKDF} was directly implemented, as per its supporting RFC \cite{RFC2898}.
In the case of \myref{DSA}, however, there were some complications.
Since it required the use of multiple precision arithmetics, we decided to use a custom patch applied to the \myref{cryptokit} library, instead of building it from scratch.
However, said patch would only work with keys of length up to 2048 bytes.
Nevertheless, the Nigori specification makes use of fixed parameters for DSA, which need keys of size 3072.
This sparked a discussion about the rigidity of the actual specification, that we will come back to in Section \ref{sec:evaluation:protocol}.
Ultimately, we decided for the current implementation to use a fixed set of keys, incompatible with the previous implementations.

\subsection{Communication}
In order for the server and client to communicate, they use a set of fixed format messages.
These are explicitly described in the Nigori RFC and were previously outlined, so instead of going over them again, we shall instead explain how communication from client to server and back is made possible by detailing on the various steps listed in Figure \ref{fig:request}.

\myfig{Preparing a request and handling a response.}{request}{1}

The most important part to understand is that these messages have a structure.
What that means in the case of this implementation is that each message is represented by an OCaml type -- which in turn is auto--generated from an Adjustable Type Definition (\myref{ATD}) annotated to produce valid OCaml types.
As such, for example, for sending a message, first the data is generated or obtained.
Then it is encrypted with either \myref{Enc} or \myref{EncDet}, appropriately.
Subsequently, we apply Base64 encoding over the content, as we cannot have certain characters for our JSON serialization.
Then an OCaml type is filled up internally with the wanted data.
Lastly, the entire message has to be serialized in a manner that is understandable by both client and server -- the protocol specifies that the server, for example, must handle JSON and should handle protobuf.

The structure of the messages is quite intricately tied to the serialization format.
In the RFC, the description of the messages was given using protobuf specifications.
However, protobufs do not currently have support for OCaml.
As such, we reverted to the second serialization format that Nigori should always support, which is JSON.
Thankfully, through a third party toolset and OCaml library, \myref{atdgen}, we were able to write up the messages as ATDs.
These were then integrated into the build step of the project to automatically create the OCaml types and implicitly, the JSON serialization and deserialization functions required.

\myfig{An excerpt of the Nigori messages \myref{ATD}.}{atd}{0.8}

The example in Figure \ref{fig:atd} shows a definition for the authentication and unregistering message.
The \myref{atdgen} tools allow us to write normal OCaml types, that can also leverage other types in the respective file.
Moreover, it allows for various annotations such as prefixing types to prevent name clashes within the file, or changing names of fields when serialized, if needed.
Finally, upon processing the \myref{ATD}, we obtain the full OCaml types, together with, in our case, JSON serializers and deserializers, that allow us to transform one type to and from strings and further manipulate them, at runtime.

The good part about this toolchain is that it allowed for each message to be crafted individually, yet be annotated with separate prefixes, so as to not generate multiple conflicting OCaml types with fields with the same name, in the same file.
Also, the fact that the functions for going back and forth through JSON were directly accessible also made development easier.
However, the toolchain also lacked something which was important in the context of this work, which is adding custom functions to be executed before serialization and, implicitly, after deserialization.
This was quite crucial, however, as we require going through and from Base64 for all content that we send as JSON.
Moreover, for those messages which also contain user data, encryption (or decryption) had to be performed, as well.
As such, wrappers for most serializers and deserializers had to be created, with both encoding and encryption added manually for each message, accordingly.
This was unfortunate, as this process was repetitive and thus error prone.

In the end, however, this potential downside ended up being an actual upside, as it helped focus more development effort into the core of the project.
Thus, functionality was added in such a way that messages could become self contained for both sending and receiving.
What this means is that, ultimately, the module that handles messages has, for each one, three functions: one for creating the message (giving it structure and adding the potential authentication pieces necessary) and one for serialization and deserialization, respectively.
These functions were designed to implicitly encapsulate all other expected pieces of functionality.

\subsection{Extra}
The most important undescribed piece of core functionality is the \myref{Nonce} module.
It encapsulates data about the nonces that the client sends along with messages that require authentication.
At the moment they contain four bytes of timestamp data and four bytes of randomness.
Together though, the combination of these two items has to be tracked in the server to prevent replay attacks -- on a per user basis.
Also, this module incorporates functionality to check the timeliness of the nonce, as that is fixed in a configuration file.
This should, however, be a server side functionality.

Nigori also has a couple of configuration items and constants kept in the shared code.
The configuration parts are, for example, the time interval for nonce lifetime or the connectivity information for the server and client, yet these can also be done dynamically.
As for constants, these are primarily the ones used for cryptography.

\section{Server}
On the server side, there is a relatively simple and direct pipeline of actions taken.
On startup, the server instantiates the internal configuration, chooses the database to be used for storage, starts up the Mirage webserver to handle requests from clients and attaches the custom request handler to it.
The handler is responsible for enforcing the protocol rules with regards to commands and structure.
Thus, it must check for validity of both endpoints and data, then ultimately unpacking the client query and translating it into a command to the underlying storage and / or into an appropriate response back to the client.

\subsection{Request Handling}
Every request the server receives it forwards to the internal handler.
There, the request path is first verified against the list of valid endpoints.
If the endpoint is valid, the request is then checked to be of according type -- for now, just POST requests are accepted.
The request body is afterwards deserialized, from the expected JSON format supported at the moment.
Then, the content pieces are decoded from Base64.
Afterwards, if authentication is required, the package signature is checked using DSA.
Finally, if all these implicit message related checks are passed, the request is ultimately turned into a direct command to the underlying database.

There is an almost one--to--one correlation between the endpoints, the communication messages and the storage system commands.
However, there is something to be said individually about endpoints and the respective responses they can generate.

% \FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | c | l | }
    \hline
    \textbf{Code} & \textbf{Entity} \\ \hline
  \hline
    200 & OK \\ \hline
    400 & Bad Request \\ \hline
    401 & Unauthorized \\ \hline
    404 & Not Found \\ \hline
    405 & Method Not Allowed \\ \hline
    409 & Conflict \\ \hline
    500 & Internal Server Error \\ \hline
  \end{tabular}
  \caption{Nigori HTTP response status codes and names.}
  \label{table:http}
\end{table}

In Table \ref{table:http} we show all HTTP response status codes that the server will generate, together with their usual entity names, for future reference as we go through the correlation between actions and potential responses.
In Figure \ref{fig:pipeline} we show data flow chart of how requests are processed internally.
As such, the handler replies with \myref{HTTP 200} on all valid endpoints, if the issued command is successful.
If an invalid endpoint is accessed, the server responds with \myref{HTTP 404}.
If methods other than POST are used, an \myref{HTTP 405} is issued.
In case the data is invalid, either from Base64 or JSON, an \myref{HTTP 400} is sent in response.
In case of internal server inconsistencies (which are codepaths that should technically not be accessed), the response is an \myref{HTTP 500}.
Finally, in case of authentication failure, an \myref{HTTP 401} is issued.
This latter case may happen for several reasons, from invalid signatures or message compositions, to replay attacks.
For the endpoints that would get back information, such as \myref{get}, \myref{get-indices} and \myref{get-revisions}, as well as for \myref{delete}, the response is an \myref{HTTP 404} if there is no item at the location pointed to by the arguments of the message.
For the \myref{register} endpoint, there is an \myref{HTTP 401} in case of invalid DSA key data, or an \myref{HTTP 409} for trying to register a user with an already registered public key.
Finally for the \myref{unregister} and \myref{put} endpoints, the special response is an \myref{HTTP 500} in case of internal failure, after users have been properly authenticated, as that should not be possible.

\myfig{Data flow chart of request processing.}{pipeline}{0.7}

\subsection{Storage}
As far as storage is concerned, the server has several requirements.
On the one hand, the server needs to hold both explicit user data, as well as certain pieces of metadata, for security purposes.
For example, nonce and timestamp data is recorded in the system, on a per user basis, to prevent potential replay attacks.
This goes in line with the requirements of the specification.

On the other hand, design wise, we had the choice of either a memory based storage system or an actual physical storage one.
In the end, we decided to opt for both, as the module system in OCaml made functorizing the aforementioned required functionality relatively easy to achieve.

This side of the server also makes use of a special module called \myref{User} which keeps track of user specific data.
At the moment, that is the user's public key and its respective hash, together with a (potentially human readable form) name and the registration date when the user was accepted into the system.
This information is useful for tracking and logging reasons.

\begin{table}[H]
  \centering
  \begin{tabular}{ |c|c|c|c| }
    \hline
    Internals & User Management & Data Management & Metadata \\\hline
    \hline
    \multirow{5}{*}{\pbox{20cm}{create}} &
    \multirow{5}{*}{\pbox{20cm}{get\_user \\ add\_user \\ delete\_user}} &
    \multirow{5}{*}{\pbox{20cm}{put\_record \\ delete\_record \\ get\_record \\ get\_indices \\ get\_revisions}} &
    \multirow{5}{*}{\pbox{20cm}{check\_and\_add\_nonce \\ clear\_nonces}} \\
    &&& \\
    &&& \\
    &&& \\
    &&& \\
    \hline
  \end{tabular}
  \caption{Nigori database functionality.}
  \label{table:database}
\end{table}

As far as functionality goes, the database module signature is split into four sections, as described in Table \ref{table:database}.

\paragraph{Internals} ~\\
This section contains the actual internal datatype used to represent the datatype, across OCaml code and one single function, to create an instance of the database.
At the moment, the creation function allows for one single parameter, an implicit name for the database, which can be used, in case of persistency, to denote the file or actual database name.
The only important note to make here is that, in order to keep state across the application, we pass the instance of the database around throughout function calls.
As the database is functorized, we decide which type we want to instantiate when we start the server, then subsequently create one and use until the webserver shut down.

\paragraph{User Management} ~\\
In order to manage user related data, we use an extra server side module, \myref{User}, which encapsulates the data that we require: the public key with which the user registers, together with a computed hash of it, for faster referencing (\myref{SHA-1} hash), as well as a (potentially human readable form) name of the user and the date it first registered with the data store, for logging and data analysis.
We use this to provide the three pieces of functionality: adding, removing and obtaining information about users, respectively.

\begin{description}
\item[\myref{add\_user}]
For registering a user with the Nigori store, the protocol requires that he provides his public key information.
As such, on the server, we store this and the other pieces of information, indexed by the \myref{SHA-1} hash of the public key, for faster access.

\item[\myref{delete\_user}]
When the user unregisters, the only requirement is a valid authentication message encapsulated in the \myref{Unregister} message.
This proves to the server the identity of the user and as such allows it to act on his behalf.
This piece of functionality removes the user's data from the store as well as his identifying information.

\item[\myref{get\_user}]
This functionality is primarily used for internal purposes, to obtain an instance of the \myref{User} module, which would imply that the user at hand is registered in the system.
For all data requests, the database is engineered in such a way to require this as proof that the authentication step has been properly handled.
\end{description}

\paragraph{Data Management} ~\\
With respect to data, the database must provide mirroring functionality to the messages that are available for data manipulation.
As such, users can add, remove and get data, as well as information about their data, such as all available indices and all the revision history information for a certain index.
All of this functionality already assumes that the user is properly authenticated.
\begin{description}
\item[\myref{put\_record}]
To add data to the database, the user must explicitly provide both an index, a revision and the respective piece of data.
If the index does not exist, then a new one, as well as the corresponding revision will be generated and the data will be added under it.
If however the index already exists, than the provided revision must not always exist in the database, as data overwrites are not allowed.

\item[\myref{delete\_record}]
In order to remove data from the store, the user can choose an explicit index and revision, to target one item of data.
However, the user can also issue a delete for an entire index, implicitly pruning the entire revision history of that index.
It is an error to provide an invalid index and / or revision.

\item[\myref{get\_record}]
For obtaining raw data from the store, the user can again, choose an explicit index and revision and be served one instance of data.
However, for history purposes, the user can also provide just the index of interest and get back the full revision history and associated data.
Yet again, it is an error to provide an invalid index and / or revision.

\item[\myref{get\_indices}]
This function provides a top level overview of the data in the store for the user, offering all indices that have data associated with them.

\item[\myref{get\_revisions}]
This function goes one level deeper, and, given an index, provides the respective revisions from the revision history associated with that index.
It is an error to provide an invalid index, however.
\end{description}

\paragraph{Metadata} ~\\
The server is required to hold certain pieces of metadata from the client requests, particularly nonce information.
The \myref{Nonce} module, as previously mentioned, comprises of simply 8 bytes of information, containing a time stamp and a single use message and functionality to check its validity.
Logging these in the server helps prevent any potential replay attacks, by disallowing requests which do not present a valid (timely) nonce.
Nevertheless, the server cannot store these indefinitely.
Hence, a configuration parameter for the server decides the amount of time that nonce data should be kept.
By default, this is set to two days of logging.
Implicitly, for management purposes, the server has two pieces of functionality related to this.
\begin{description}
\item[\myref{check\_and\_add\_nonce}]
This function serves the dual purpose of both verifying the validity of an incoming nonce from a client, and, if proper, to add it to the list of seen nonces from the respective client.

\item[\myref{clear\_nonces}]
Whenever this function is called, any outdated nonces are purged from the database.
\end{description}

\section{Client}
For the purposes of this project, we have implemented a simplified version of the client.
This was built on top of Mirage's \myref{ocaml-cohttp} HTTP client.
It handles all of the mechanics the protocol describes, with the exception of proper client--side revisioning (conflict resolution).
In the current version, it can be used though to trigger all of the available functionality from the server.
Moreover, the used messages are structured according to the specification, then, as previously described, serialized down to JSON, with the data properly encoded using Base64 encoding and encryption applied on all user data.

Some interesting factors worth mentioning in the development of the client revolve around the actual communication and interaction between client and server.
Due to the available REST interface on the server, the client has to be aware of the available endpoints.
As such, these become pushed towards shared code.
Moreover, on the message side, packing, signatures and verification, encoding and decoding, as well as serialization and deserialization were all pushed in the shared section, such that the messages would be self--contained, yet both the client and the server could access the respective functionality that they needed.
Overall, due to the complex set of actions that are performed on messages, getting the client to work properly was probably the most intensive part of the project.
Nevertheless, the client in itself is relatively small, code--wise.
However, this is purely due to the amount of code that was moved into the shared section, out of necessity and choices.

In order to function properly, the client required all components to work properly, both individually and together.
As such, most difficulties were experienced on the integration step of the work.
The first problem was that the \myref{DSA} key generation had to work properly, and, as mentioned, this was quite cumbersome.
Moreover, proper \myref{DSA} signatures on top of messages and proof that the respective signature verification mechanism worked right was needed.
Lastly, message structure had to be fixed and useful for both client queries and potential server responses, together with all the aforementioned functionality encapsulated in the \myref{Message} module.
However, thanks to the proper methodology applied across the project, the integration work proved quite successful, from the side of what was being developed.
Most problems that did arise came from the external tools being used, from the faulty \myref{DSA} implementation, to the lacking functionality in the \myref{ATD} generation library.
We discuss some more aspects of the usability of these in Chapter \ref{chapter:evaluation}.

\section{JavaScript implementation}
In order to provide a working Javascript client, we used a toolchain and set of libraries called \myref{js\_of\_ocaml}.
It allows for OCaml compiled bytecode to be directly converted into Javascript code.
Nevertheless, certain features that are generally platform dependent do not function properly.
As such, in order to obtain a working client, we had to functorize quite a portion of our code base.
Nevertheless, this only implied abstracting the module signatures and instantiating modules that needed them, accordingly.

For example, the most important pieces of functionality that we could not directly use, as we had them in OCaml, were the HTTP client, the cryptography parts and the JSON serialization.
While that may sound daunting, in reality, it involved considerably less effort than expected.
By functorizing the respective libraries, we were able to create other OCaml modules for each of the pieces we needed to replace to generate valid Javascript code.
For instance, the \myref{js\_of\_ocaml} libraries come with a built in module called \myref{XmlHttpRequest}, which allows for HTTP requests.
As such, abstracting the functionality needed and provided by our client was as simple as replacing the Mirage client with a separate indirection module that could be functorized either with the \myref{ocaml-cohttp} client, or the respective Javascript compliant one, depending on the targeting platform.
Similarly, while we could keep the entire \myref{ATD} generated OCaml types, we replaced the JSON serialization, which uses OCaml IO functionality, with a \myref{js\_of\_ocaml} module called \myref{Deriving\_Json} which implicitly allowed for marshalling objects to and from JSON, in a safe manner.

For the cryptography part, however, we had to work a bit more.
Some of the internals of the \myref{cryptokit} library use functionality, such as multiple precision arithmetics, which are platform dependent.
As such, we had to find a suitable replacement.
To that end, we used an actual Javascript library, called \myref{jsCrypto} \cite{jscrypto}, which conveniently provided all of the primitives we required, to be used in Javascript.
Nevertheless, we still needed to create suitable OCaml modules that would act as indirection layers.
Hence, for all the primitives we used, \myref{SHA-1}, \myref{HMAC}, \myref{AES}, \myref{DSA} and \myref{PBKDF}, we abstracted their interfaces, into indirection OCaml types.
Afterwards, we replaced the previous modules with functorized implementations of these interfaces and we built the ones that would use the Javascript version.
For the actual building, we used a functionality of \myref{js\_of\_ocaml} that allowed for explicitly calling other Javascript functions that were already loaded.
Hence, we respectively had a double indirection, once through the OCaml interface and again towards directly calling the \myref{jsCrypto} functions.

Albeit taking quite a bit of tweaking to fully understand the back and forth conversion between OCaml types and implicit Javascript objects, in the end, we were left with a functioning client for Nigori.
Moreover, the way in which this was obtained, was rather semi--automated, considering most of the requirements, after understanding the workings, was functorizing the underlying pieces that needed to be swapped out.
Thus, the outcome with regards to managing the protocol implementation was as we expected.
We were able to develop the core of the protocol, in essence, only once, and then, using appropriate substitutions of the lower layer of functionality, essentially retarget the entire development towards deployment on a different platform.
