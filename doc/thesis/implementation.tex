\chapter{Design and Implementation} \label{chapter:implementation}
Nigori, as a system, is basically an encrypted index--value store, built for the cloud.
As such, the big picture consists of primarily two components: a client and a server.
Nevertheless, due to the nature of the interactions between these two pieces, quite a sizeable chunk of code ends up being shared, particularly the communication and some of the encryption mechanics.
This chapter is modeled after the actual implementation, showing the shared code, and the client and the server in separation, while also discussing the interactions between them.

In adition to describing the base OCaml implementation, this chapter also provides details about the testing done to ensure the application performs according to specifications.
Lastly, the separate compilation from OCaml code to JavaScript is also detailed, along with both what it requires as well as what it entails.

TODO: potentially add a figure or set of figures showing module dependencies and individual pieces, to help with direct naming.

\section{Shared}
The coresponding section of the codebase for shared code actually represents the core of the application.
For the most part, components in this category are used by both the server and the client.
As such, they have been implemented according to the simplicity and modularity principles mentioned in the previous chapter.
Implicitly, this made using this core in both client and server, a straightforward venture.
Moreover, it also allowed for an easy prototyping of the javascript version of the client, as well, since the core functionality was merely functorized to match the targetted platform.

\subsection{Crypto}
As listed in the initial draft for Nigori, several cryptographic functions are required for the operation of the protocol.
These functions were built on top of the \myref{cryptokit} library for OCaml and abstracted into their individulal modules, each capable of providing the expected functionality as well as being tested or verified accordingly.

As such, we had several base primitives that were required such as \myref{SHA-1}, \myref{HMAC}, \myref{AES}, \myref{DSA} and \myref{PBKDF}.
We use \myref{SHA-1} for hashing purposes, both implicitly (such as in \myref{PBKDF}), but also explicitly (for example, on \myref{DSA} keys in messages).
\myref{HMAC} is used solely on the encryption and decryption steps of indices, revisions or actual data.
Similary, \myref{DSA} is used primarily for authentication.
In the client, we create the public / private key pairs for a particular user.
Afterwards, we generate and introduce a message signature, inside packets that are or contain an authentication message themselves.
Finally, on the server, the signature is verified, accordingly, using the user's public key, that is stored as part of his identity.
Lastly, \myref{PBKDF} is used, together with the users's master password, on the client, to generate specific keys to be used for user authentication, encryption, MACs or as initialization vectors in \myref{AES}.

Of all of the above, \myref{SHA-1} and \myref{HMAC} came with the OCaml \myref{cryptokit} library.
\myref{AES} was also provided, but had to be tweaked according to the protocol specification.
The \myref{PBKDF} was directly implemented, as per its supporting RFC \cite{RFC2898}.
In the case of \myref{DSA}, however, there were some complications.
Since it required the use of multiple precision arithmetics, we decided to use a custom patch applied to the \myref{cryptokit} library, instead of building it from scratch.
However, said patch would only work with keys of length up to 2048 bytes.
Nevertheless, the Nigori specification makes use of fixed parameters for DSA, which need keys of size 3072.
As such, the current implementation uses a fixed set of keys, incompatible with the previous implementations.

\subsection{Communication}
In order for the server and client to communicate, they use a set of fixed format messages.
These are explicitly described in the Nigori RFC and was previously outlined, so instead of going over them again, we shall instead explain how communication is made possible by showing what happens to messages from creation, to sending, to receiving and reading.

TODO: add picture of how an individual message is formed.

The most important part to understand is that messages are structured.
What that means in the case of this implementation is that each message is represented by an OCaml type -- which in turn is auto--generated from an Adjustable Type Definition (ATD) annotated to produce valid OCaml types.
As such, for example, for sending a message, first a type is filled up internally with the wanted data.
Afterwards, the content of the message has to be properly encoded, for transmission.
In our case, we use Base64 encoding, as provided by the Mirage libraries.
Lastly, the entire message has to be serialized in a manner that is understandable by both client and server -- the protocol specifies that the server, for example, must handle JSON and should handle protobuf.

The structure of the messages is quite intricately tied to the serialization format.
In the RFC, the description of the messages was given using protobuf specifications.
However, protobufs do not currently have support for OCaml.
As such, we reverted to the second serialization format that Nigori should always support, which is JSON.
Thankfully, through a third party toolset and OCaml library, \myref{atdgen}, we were able to write up the messages as ATD.
These were then integrated into the build step of the project to automatically create the OCaml types and implicitly, the JSON serialization and deserialization functions required.

The good part about this toolchain is that it allowed for each message to be crafted individually, yet be annotated with separate prefixes, so as to not generate multiple conflicting OCaml types with fields with the same name, in the same file.
Also, the fact that the functions for going back and forth through JSON were directly accessible also made development easier.
However, the toolchain also lacked something which was important in the context of this work, which is adding custom functions to be executed before serialization and, implicitly after deserialization.
This was quite crucial, however, as we require Base64 encoding of all message content, before the serialization.
Moreover, for those messages which also contain user data, encryption (or decryption) had to be performed.
As such, wrappers for most serializers and deserializers had to be created, with both encoding and encryption added manually for each message, accordingly.
This was unfortunate, as, this process was repetitive and thus quite error prone.

In the end, however, this potential downside turned into an upside, as it helped focus more development effort into the core of the project.
Thus, functionality was added in such a way that messages could become self contained for both sending and receiving.
What this means is that, ultimately, the module that handles messages has, for each one, three functions: one for creating the message (giving it structure and adding the potential authentication pieces necessary) and two for serialization and deserialization.
Hence, even the required encryption on uploading content and decryption on responses was encapsulated in the message implementation directly, as was the \myref{DSA} signature for the messages, even if they were themselves just a field in an encompassing message.

\subsection{Extra}
The most important undescribed piece of core functionality is the \myref{Nonce} module.
It encapsulates data about the nonces that the client sends along with messages that require authentication.
At the moment they contain four bytes of timestamp data and four bytes of randomness.
Together though, the combination of these two items has to be tracked in the server to prevent replay attacks -- on a per user basis.
Also, this module incorporates functionality to check the timeliness of the nonce, as that is fixed in a configuration file.
This should, however, be a server side functionality.

Nigori also has a couple of configuration items and constants kept in the shared code.
The configuration parts are, for example, the time interval for nonce lifetime or the connectivity information for the server and client, yet these can also be done dynamically.
As for constants, these are primarily the ones used for cryptography.

\section{Server}
On the server side, there is a relatively simple and direct pipeline of actions taken (TODO: figure).
On startup, the server instantiates the internal configuration, the database to be used for storage, starts up the Mirage webserver to handle requests from clients and attaches the custom request handler to it.
Afterwards, every request the server receives is first verified against the list of available endpoints.
If the endpoint is valid, the request is forwarded to the handler, as is.
Alternatively, the default handler action is issued, which is to respond with a \myref{404 Error} back to the client (TODO: figure).
The handler is responsible for properly unpacking the client query and translating it into a command to the underlying storage and / or into an appropriate response back to the client.

TODO: figure for server components

TODO: figure for endpoint validation process

\subsection{Request Handling}
On the request handler side, every request at a valid endpoint is checked to be of according type (for now, just POST requests are accepted).
If all is proper, the request body is unpacked.
First the raw data is deserialized (at the moment, from JSON format).
Then, the content is decoded from Base64.
Afterwards, if authentification is required, the package signature is checked using DSA.
Finally, if all these implicit message related checks are passed, the request is ultimately turned into a direct command to the underlying database.

TODO: figure with data flow

There is an almost one--to--one corelation between the endpoints, the communication messages and the storage system commands.
However, there is something to be said individually about endpoints and the respective responses they can generate.
Below you will find a table of all HTTP response status codes that the server will generate, followed by the correlation between actions and potential responses.

% \FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | c | l | }
    \hline
    \textbf{Code} & \textbf{Entity} \\ \hline
  \hline
    200 & OK \\ \hline
    401 & Unauthorized \\ \hline
    404 & Not Found \\ \hline
    405 & Method Not Allowed \\ \hline
    409 & Conflict \\ \hline
    500 & Internal Server Error \\ \hline
  \end{tabular}
  \caption{Nigori HTTP response status codes and names.}
\end{table}

All endpoints reply with \myref{HTTP 200} if the issued command is successful.
If an invalid endpoint is accessed, the server responds with \myref{HTTP 404}.
If methods other than POST are used, an \myref{HTTP 405} is issued.
In case of internal server inconsistencies (which are codepaths that should technically not be accessed), the response is an \myref{HTTP 500}.
Finally, in case of authentication failure, an \myref{HTTP 401} is issued.
This latter case may happen for several reasons, from invalid signatures or message compositions, to replay attacks.

For the endpoints that would get back information, such as \myref{get}, \myref{get-indices} and \myref{get-revisions}, as well as for \myref{delete}, the response is an \myref{HTTP 404} if there is no item at the location pointed to by the arguments of the message.

For the \myref{register} endpoint, there is an \myref{HTTP 401} in case of invalid DSA key data, or an \myref{HTTP 409} for trying to register a user with an already registered public key.

Finally for the \myref{unregister} and \myref{put} endpoints, the special response is an \myref{HTTP 500} in case of internal failure, after users have been properly authenticated, as that should not be posible.

\subsection{Storage}
As far as storage was concearned, the server has several requirements.
On the one hand, the server needs to hold both explicit user data, as well as certain pieces of metadata, for security purposes.
For example, nonce and timestamp data is recorded in the system, on a per user basis, to prevent potential replay attacks.
This goes in line with the requirements of the specification.

On the other hand, design wise, we had the choice of either a memory based storage system or an actual physical storage one.
In the end, we decided to opt for both, as the module system in OCaml made functorizing the aforementioned required functionality relatively easy to achieve.

This side of the server also makes use of a special module called \myref{User} which keeps track of user specific data.
At the moment, that is the user's public key and its respective hash, together with a (potentially human readable form) name and the registration date when the user was accepted into the system.
This information is useful for tracking and logging reasons.

As far as functionality goes, the database module signature is split into four sections (we provide the OCaml signatures for ease of understanding):
\begin{description}
  \item[Datatype] the internally used datatype and the instantiation function
  \begin{itemize}
    \item \myref{type t} \\
    This represents the internal datatype that, in this case, will be used as an instance of the module.
    In effect, each implementation will have this as the main storage component, be it an in--memory OCaml \myref{Hashtbl}, some form of an SQL wrapper, a wrapper over the HTML5 LocalStorage, or whatever else would be required.
    \item \myref{val create : unit -> t} \\
    This is used to actually instantiate the database.
    When the server starts up, before it starts the webserver and request handler, it instantiates a database that will be used across all requests.
    In light of this datatype usage, the remainder of the module functions all take a named parameter \myref{database:t}, which is supposed to be the instance of the database that the server has access to.
  \end{itemize}

  \item[User management] the overarching user related functions for allowing a database to handle data for multiple users at the same time
  \begin{itemize}
  \item \myref{val add\_user : database:t -> pub\_key:User.public\_key -> pub\_hash:User.hash -> bool} \\
  In order to even begin holding user data, the server must first register the respective user(s).
  This function acomplishes exactly that.
  It receives the public key the user whishes to be authenticated with, \myref{pub\_key:User.public\_key}, as well as a hash of that respective key, \myref{pub\_hash:User.hash}.
  This data would then be stored internally, for later lookup.
  Since the hash is used in messages instead of the actual key (for space efficiency reasons), we implicitly take the liberty us using this hash as the main keying factor across the storage.
  This is, however, an implementation decision, which can potentially affect security, yet, since the protocol specification considers this enough for message transmission, the server can easily make the same assumptions.

  What is more, this hash is supposed to be computed from the actual public key and, moreover, should be generated in the same way by both the client and server.
  However, for the versatility of the specification, and, implicitly, the implementation, it is left, as often as possible, as a parameter, as well as used as a hash in messages.
  Nevertheless, the implementation of the \myref{User} module, allows for the automatic generation of the hash, assuming the public key is given.

  Given the above parameters, the database attempts to register the user internally.
  If this is possible, it does so and returns \myresult{true}.
  Alternatively, it returns \myresult{false}.

  \item \myref{val get\_user : database:t -> pub\_hash:User.hash -> User.t option} \\
  In order to actually get the information of a particular user, the database has to hold all the information necessary for an instance of the \myref{User} module internally.
  Across both the in--memory version and the SQL one, this is kept keyed by the hash of the public key of that respective user.
  Thus, the parameter \myref{pub\_hash:User.hash} is used for lookup to get all other user related data that might be useful for authentication purposes.

  If a user matching the respective hash exists, the function wraps up the \myref{User} instance into an option, returning \myresult{Some(user)}.
  Alternatively, it returns \myresult{None}.

  \item \myref{val have\_user : database:t -> pub\_hash:User.hash -> bool} \\
  A simplified version of the \myref{get\_user} function, this simply checks if the respective user is registered on this server.
  The respective \myref{pub\_hash:User.hash} is again used for lookup.

  The function returns appropriately \myresult{true} if the user exists and \myresult{false} otherwise.

  \item \myref{val delete\_user : database:t -> user:User.t -> bool} \\
  This function servers as the oposite of the \myref{add\_user} one.
  It will effectively deregister the respective user from the server.
  Moreover, all of his respective data should also be deleted at this time, as well.

  An important note is that this function receives a \myref{user:User.t} as a parameter.
  This is a design choice that should server as an extra level of internal security.
  Since this should be a rare occurrence, the potential extra query that might be required should be inconsequential (performance wise) and thus justify the divergence rom diverging from the standard of this module's overall signature, of using the public key hash as a lookup key.
  As such, the database requires that explicit existence of a \myref{User}, which can only be obtained after proper authentication in the system.

  If the function is successful and the user can be removed, the function returns \myresult{true}.
  Alternatively, if the user does not exist, or there is some internal inconsistency, the function returns \myresult{false}.

  \item \myref{val get\_public\_key : database:t -> pub\_hash:User.hash -> User.public\_key option} \\
  This is more of a convenience function for directly accessing a user's public key, assuming it is available, as in the user is registered.
  If so, the function wraps it into an option, returning \myresult{Some(key)}, alternatively returning \myresult{None}.
  \end{itemize}
  \item[User data] the actual functionality for handling the data of individual users; all of these take an extra parameter, the actual instance of the \myref{User} module for which actions will be performed, signifying that that respective user is authenticated in the system
  \begin{itemize}
  \item \myref{val put\_record : database:t -> user:User.t -> index:string -> revision:string -> data:string -> bool} \\
  This function is to be used to add content into the database, for a given user.
  As the user data is supposed to be basically a two level index--value store, for each respective user, this function is parametrized by everything needed to identify all components accordingly.
  The user stores data under certain indices, denoted here by the \myref{index:string} parameter.
  Under each such \myref{index}, there can be multiple revisions, as denoted by the  \myref{revision:string} parameter, each with its own version of the respective content, as finally denoted by the \myref{data:string} parameter.

  The function respectively returns \myresult{true} if the action could be performed. However, if the user is not authenticated, or the revision exists and the data is inconsistent, then the function returns \myresult{false}.

  \item \myref{val get\_record : database:t -> user:User.t -> index:string -> ?revision:string option -> unit -> Messages\_t.revision\_value list option} \\
  This function is technically the getter counterpart of the \myref{put\_record} function.
  The \myref{index:string} parameter specifies an index inside the store to access.
  If the \myref{revision:string option} parameter can specify an explicit \myref{revision} that is required.

  With no explicit \myref{revision} required, the function gathers a list of all revisions under the respective \myref{index}. Alternatively, it wraps the explicit result in a list of one element. Assuming all goes well, the function further wraps this list in an option, returning \myresult{Some(list)}, otherwise, returning \myresult{None}. The result list is directly encoded in message format, hence the \myref{Message\_t} in the output of the signature, which is the autogenerated OCaml type, from the ATD.

  \item \myref{val get\_indices : database:t -> user:User.t -> string list option} \\
  Since data for every user will be kept in some form of two level index--value format, this function is tasked with simply getting all the \myref{index} names registered for a particular user.

  If the user exists, the function returns the list, potentially empty, wrapped in an option, \myresult{Some(list)}, else, it returns \myresult{None}.

  \item \myref{val get\_revisions : database:t -> user:User.t -> index:string -> string list option} \\
  This function goes one level deeper than the \myref{get\_indices} function.
  For a particular user, this function will return all the respective \myref{revision} names under the given \myref{index} given as parameter.

  If the user exist and has the given \myref{index} registered, the function returns the potentially empty list wrapped in an option, myresult{Some(list)}, or alternatively, \myresult{None}.

  \item \myref{val delete\_record : database:t -> user:User.t -> index:string -> ?revision:string option -> unit -> bool} \\
  This function allows user to also remove content they have uploaded.
  Given the \myref{index} and optionally the \myref{revision} parameters, this function can delete either one particular revision or all revisions under the given \myref{index}.

  If the \myref{index} and optionally, the \myref{revision} exist and the user is properly authenticated, the operation returns \myresult{true}, otherwise, \myresult{false}.

  \end{itemize}
  \item[Metadata] the nonce related functionality, responsible for ensuring timeliness of messages and replay protection
  \begin{itemize}
  \item \myref{val check\_and\_add\_nonce : database:t -> nonce:Nonce.t -> pub\_hash:User.hash -> bool} \\
  This function is responsible for checking that the respective \myref{Nonce} is acceptible for the user denoted by the \myref{pub\_hash:User.hash} parameter.
  The latter is implicitly supposed to be used to match an existing user and also lookup in whichever datastructure will keep track of the nonces.

  If the nonce is acceptable in the context of this respective user, the function returns \myresult{true}, otherwise, it returns \myresult{false}.

  \item \myref{val clear\_old\_nonces : database:t -> unit} \\
  This function is supposed to be used to clear the internal tracking of nonces older than some server dependant amount of time.
  The specification of the system does not give explicit suggestions for this, as this can depend from server to server, typically based on experianced load.
  \end{itemize}
\end{description}

Despite the ease that OCaml brought towards the modularity aspect of the implementation, there are still certain design differences between the actual in--memory storage version of the database and the physical storage one.
Moreover, there were also certain issues and quirks that took a certain amount of time to either identify and / or get around.
As such, we also include a discussion on these elements in what follows:
\begin{description}
  \item[Memory] the in--memory variant of the database makes use of the native library OCaml \myref{Hashtbl} datastructure.
  As expected, this is a hash table with parameterized keys and values.
  For this version of the database, we use an OCaml record to represent the internal \myref{type t}.
  This record keeps track of the \myref{User} related data (public keys and hashes), the actual data (\myref{indices}, \myref{revisions}, myref{data}) and finally, the metadata (\myref{nonces}).
  As previously mentioned, for security, we index the \myref{User} data, by the \myref{hash} and then the actual user data by instances of the \myref{User} module.
  Lastly, the only non--trivial or unexpected choice is related to the storing of nonces.
  While the \myref{Nonce} module allows for serializing instances to strings, OCaml does not provide a default \myref{Set} with O(1) access time.
  In absence of such, we store nonces, on a per user basis into yet another \myref{Hashtbl}, indexed from actual \myref{Nonce} instances to boolean values.

  \item[SQL] TODO: Quirks, problems and limitations.
\end{description}

\section{Client}
For the purposes of this project, we have implemented a simplified version of the client.
This was built on top of Mirage's \myref{ocaml-cohttp} HTTP client.
It handles all of the mechanics the protocol describes, with the exception of proper client--side revisioning (conflict resolution).
In the current version, it can be used though to send any of the messages normally available.
These are, as per the specification, serialized down to JSON, as previously described, with the data properly encoded using Base64 encoding.
Moreover, the security mechanisms described in the specifications are also applied, when required.

Some interesting factors worth mentioning in the development of the client revolve around the actual communication and interaction between client and server.
Due to the available REST interface on the server, the client has to be aware of the available endpoints.
As such, this pushes these towards them being available in a more shared location.
Moreover, due to the way in which messages are packaged, signed, serialized and eventually marshalled on the client side and ultimately undone on the server side, getting the client to work properly was probably the most intensive part of the project.
Despite the client in itself being relatively small, code--wise, that is purely due to the amount of code that was moved into the shared section, out of necessity.

As such, the client required all components to work properly, both individually, and together, before it would function right.
\myref{DSA} key generation had to work.
Message structure had to be fixed and useful for both client queries and potential server responses, together with the respective JSON (de)serialization.
Proper \myref{DSA} signatures on top of the respective messages and proof that the respective \myref{DSA} signature verification worked right needed to be accomplished.
However, due to the proper methodology applied across the project, the integration work did not prove much hassle, from the side of what was being developed.
Most problems that did arise came from the external tools being used, as previously detailed.

\section{Testing}
Short of using formal verification tools for the intricate pieces of the Nigori protocol, we resorted to pure functional testing.
As such, we took the respective library offered functionality as given and tested everything that was built on top of it accordingly.
Each of the cryptographic primitives was tested individually.
Furthermore, as per the client requirements, these primitives were also tested in combination and the results compared against running the same operations with external tools.

Also, the corresponding encryption and decryption steps were tested to work well combined.
Likewise, the respective JSON serialization and deserialization were also tested to perform as expected, when combined with the remainder of the system functionality (encoding / encryption).

Similarly, the server's database implementations for both the in--memory version and the SQLite backed one were tested in parallel, thanks to the module functorization capabilities of OCaml.
Thus, all of the provided functionality from the database was tested, under the various scenarios in which messages could be exchanged and commands could be subsequently issued, in light of those exchanges.
As such, proper test coverage was achieved on all code paths that were feasible.

\section{JavaScript implementation}
TODO: Discuss how the semi--automated version of the client to JS is done.

TODO: Explain manual pieces and how they had to be glued together.

TODO: Potentially show how, in the end, after boilerplate is overcome, changes to core get propagated to client, leading to fast prototyping.
